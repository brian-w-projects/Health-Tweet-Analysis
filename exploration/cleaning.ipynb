{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame()\n",
    "for file in os.listdir('data'):\n",
    "    raw = pd.read_csv(f'data/{file}', sep='|', names=['id', 'date', 'tweet'], encoding='ISO-8859-1') \\\n",
    "        .assign(target=re.sub(r'(?i)(\\w+)health.*', r'\\1', file))\n",
    "    data = pd.concat([data, raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63028, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.column]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('..','stop_words'), 'r') as reader:\n",
    "    stop_words = reader.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "\n",
    "class Stemmer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.en_nlp = spacy.load('en_core_web_sm')\n",
    "        self.stemmer = nltk.stem.PorterStemmer()\n",
    "    \n",
    "    def __call__(self, tweet):\n",
    "        pattern = re.compile(r'([@#A-Za-z]{2,})')\n",
    "        return [self.stemmer.stem(t.norm_) for t in self.en_nlp(tweet) if pattern.match(t.norm_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    ColumnSelector('tweet'),\n",
    "    TfidfVectorizer(tokenizer=Stemmer(), stop_words=stop_words, \n",
    "                    min_df=1, max_df=0.5, max_features=20000, strip_accents='ascii'),\n",
    "    LatentDirichletAllocation(n_components=25, learning_method='batch', max_iter=25, random_state=0)\n",
    ")\n",
    "\n",
    "assignment = pipe.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "['mental', 'health', 'gonna', 'allergi', 'suicid', 'ill', 'polio', 'canadian', 'solv', 'doctor']\n",
      "Topic 1\n",
      "['mer', 'viru', 'fish', 'born', 'air', 'pollut', 'everyday', 'deadli', 'daili', 'saudi']\n",
      "Topic 2\n",
      "['health', 'insur', 'obamacar', 'law', 'care', 'exchang', 'rt', 'plan', 'state', 'report']\n",
      "Topic 3\n",
      "['weight', 'food', '@cynthiasass', 'rt', 'gonna', 'eat', '@goodhealth', 'fat', 'calori', 'diet']\n",
      "Topic 4\n",
      "['teen', 'smoke', 'kid', 'drive', 'gonna', 'pregnant', 'studi', 'shot', 'alcohol', 'abus']\n",
      "Topic 5\n",
      "['gonna', 'autism', 'play', 'game', 'role', 'social', 'studi', 'knee', 'media', 'nh']\n",
      "Topic 6\n",
      "['rt', 'trial', 'pharma', '@pharmalot', 'gonna', 'propos', 'drug', 'ebola', 'amp', 'anim']\n",
      "Topic 7\n",
      "['approv', 'fda', 'longer', 'beauti', 'skin', 'painkil', 'gonna', 'food', 'power', 'hair']\n",
      "Topic 8\n",
      "['court', 'cigarett', 'rule', 'abort', 'tobacco', 'smoker', 'suprem', 'pill', 'quit', 'gener']\n",
      "Topic 9\n",
      "['flu', 'bird', 'cell', 'scientist', 'stem', 'gonna', 'vaccin', 'china', 'outbreak', 'global']\n",
      "Topic 10\n",
      "['gonna', 'adhd', 'infant', 'hpv', 'type', 'job', 'diagnos', 'rt', 'diabet', 'vaccin']\n",
      "Topic 11\n",
      "['age', 'blog', 'dementia', 'gonna', 'pressur', 'blood', 'older', 'rt', 'addict', 'athlet']\n",
      "Topic 12\n",
      "['rt', 'nh', 'gonna', '@gdnhealthcar', '@guardian', 'gap', 'north', 'speak', 'care', 'spot']\n",
      "Topic 13\n",
      "['recip', 'gonna', 'day', 'tri', 'thi', 'healthi', 'fruit', 'salad', 'veggi', 'idea']\n",
      "Topic 14"
     ]
    }
   ],
   "source": [
    "sorting = np.argsort(pipe.get_params()['latentdirichletallocation'].components_, axis=1)[:, ::-1]\n",
    "\n",
    "feature_names = pipe.get_params()['tfidfvectorizer'].get_feature_names()\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('output.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for topic in range(sorting.shape[0]):\n",
    "        print(f'Topic {topic}')\n",
    "        print([feature_names[sorting[topic, i]] for i in range(10)])\n",
    "        writer.writerow(feature_names[sorting[topic, i]] for i in range(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
